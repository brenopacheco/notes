# Flex introduction

This is an introduction to what Flex (Fast LEXical analyzer generator) does.
It brings some of Flex concepts the way I'd like to have been taught.
This text is for you who have read some things about Flex and did not quite
grasp what it does by itself.

## What Flex does

You may have heard that Flex is scanner/tokenizer/lexical analyzer generator.
That's true, but only half of it. Here's what it actually does:

Flex takes a .l file as input and spits out a .c file.

1. The .l file is something we write with rules for how to match pieces of text.
2. The .c gives us a function for getting getting matches, one at a time (yylex).
3. Every time we match text against a rule, a piece of code we defined in .l runs.

And that's basically it.

Flex doesn't come with a builtin token concept. It simply provides us with a
function to match pieces of text in a stream using regexes, piece by piece.
The token concept will be introduced by us later, but is not something coming
FROM flex and is certainly not required to understand or to write a Flex
generated scanner. Let's see what is this .l file first:

## The .l file

1. The .l file specifies regular expressions we wish to match.
2. The .l file also defines WHAT TO DO when these regex are matched.
3. These are called rules.
4. We write rules as follows:

```lex
    [0-9]+      { /* run this when a NUMBER is matched */ }
    [a-zA-Z]+   { /* run this when a WORD   is matched */ }
    "+"         { /* run this when a +      is matched */ }
```

The .l file however has 3 sections separated by %%, because
we may want to include things at the top and at the bottom of
our the Flex generated .c file. The rules go into the 2nd section.

```lex
    %{ /* include this comment on top of generated .c file*/ %}
    %%
    [0-9]+      { /* do something when a number is matched */ }
    [a-zA-Z]+   { /* do something when a word   is matched */ }
    "+"         { /* run this when a +      is matched */ }
    %%
    /* include this comment on bottom of generated .c file */
```

_Why the hell is the 1st section code surrounded by %{ %} ?_

... Well, the 1st section is where we define some Flex %options
and maybe give a name to some of our regexes. We can do more
stuff here, but that's more or less the jist.

We usually use the first section to include some header and
forward declare some function if we need to. The 3rd section
may be used to define some function or even include a main
for compiling our .c file as an executable by itself.

The example below may clarify this:

```lex
    %{
        #include "mycoolheader.h"
        void somefunction();
    %}

    %{
        /* we can make more of these code snippets if we want to */
        /* below we use the yylineno option, which will make flex
        declare and keep track of a variable called yylineno that
        holds the line number for the current matched token. */
    %}

    %option yylineno
    mycoolregex wow(such|regex)+{3}

    %%
    {mycoolregex}  { /* do something when shibe is matched    */ }
    [0-9]+         { /* do something when a number is matched */ }
    [a-zA-Z]+      { /* do something when a word   is matched */ }
    "+"            { /* run this when a +      is matched     */ }
    %%

    void somefunction() {
        /* here i could define it */
    }

    void main(int argc, char argv[]) {
        /* maybe declare a main! */
    }
```

### The .c file

_Alright we've seen what the .l consists of but what does Flex provides us?_

The .c file will provide us with a few things. Mainly:

1. int yylex() function
2. char\* yytext variable
3. FILE\* yyin variable

The yyin is actually the pointer to where our program will read it's stream from.
By default, it points to the STDIN, but that can be changed to point to a file or
somewhere else.

The yylex() is THE big thing here. It is the function that will read our stream
and compare to the Rules we defined. When it does find a match, it will copy the
matched string into a new memory location and make _yytext_ point to it. After
that, it will run the code we specified in our Rule, returning whatever int we
told it to return.

For example, suppose we wrote the following rules:

```c
    [0-9] { printf("%s", yytext); return 1;}
    [a-z]+ { printf("%s", yytext); return 2;}
```

And suppose the input stream is "abc2a".

We can call yylex() 3 times:

1. call yylex() returns 1, yytext -> 'abc\0'
2. call yylex() returns 2, yytext -> '2\0'
3. call yylex() returns 1, yytext -> 'a\0'
4. call yylex() returns 0 (indicating EOF)

With each call to yylex() the content of yytext is printed on the screen.

Thats it. From here you can define whatever you want to do with the
strings you match. Convert numbers to integers and store, uppercase
words, count the words you find, I dunno.

#### Some more functionalities

Additionally, we have some more utilities, like

1. int yyleng :: the length of the lexeme matched
2. char\* yyless(k) :: returns the first k characters in yytext
3. void yyterminate() :: makes yylex() return 0. terminates scanning.
4. FILE\* yyout :: the pointer to the file to output (def. stdout)
5. macro ECHO :: writes yytext to wherever yyout is pointing to

yyleng is simply the length of the yytext string, and yyless an
utility function to return this text partially. There are cases we
find matches in our stream that weren't supposed to be there,
and we want to stop the scanning. yyterminate() will do just that,
make yylex indicate that it has reached EOF.

Flex also provides this ECHO macro we can use to print our matched yytext,
and it defines a FILE\* yyout pointing to STDOUT by default. We can, again,
overwrite that.

#### The yywrap error

The .c file Flex generates expects by default that YOU provide an implementation
of a function called yywrap. This function is actually run by yylex() when it
reaches EOF. Why? Because we may want to start scanning another file when the
first one is done, and yywrap would make yyin point to this new file and
yylex would be run again. the yywrap will return 0 if the scanning is to continue
and 1 or something else if the scanning is supposed to be done.

Now, you can compile your .l file like this:

```sh
    flex myfile.l -ltl
```

The -ltl flag tells flex to use a default implementation of yywrap
(which does nothing). You could also have defined the function .l file
or have placed the %option nnoywrap.

#### Things not covered

I'll not go into some other features Flex has, such as STATES (which are
ways of defining subscanners within our scanner).

### The tokens

Again, the concept of tokens is not really comming from Flex itself. What
is a token in a Flex generated program, how we define and where we store
it depends on US. But Flex is usually coupled to Bison (which it doesn't
necessarily need to be) and thefere the token thing is generally introduced
as being part of it. I'll try to introduce it here from the Bison point of
view, so that we can write some examples.

A token is something we identify in our code/stream. It has a type and a regex
for matching it in the stream. The possible values a token may hold are called
lexemes, and these have a data type.

| token type  | token value (lexeme)  | regex        | data type   |
| :---------- | :-------------------- | :----------- | :---------- |
| KEYWORDS    | int, while, for ...   | (int...)     | char\*      |
| IDENTIFIERS | main, i, j ...        | [a-z]        | char\*      |
| CONSTANTS   | 10, 20, 1554.5 ...    | [0-9]+       | int, double |
| STRINGS     | "hello", "world" ...  | \".\*\"      | char\*      |
| SPECIAL     | (, ), {, }, [, ], ... | [(){}[]]...] | char        |
| OPERATORS   | +, /, -, \*,...       | [+-/*]       | char        |

When using flex, we usually want to define out token types. We can do
so using an enumerator. I'll be using Bison's convention for naming here
and calling the enum yytokentype:

```c
    enum yyokentype {
        OPERATOR = 99,
        NUMBER   = 98,
        IDENTIFIER = 97,
        ...
    }
```

During our scanning, when a token is found we usually store it's value
in some variable. By convention, this variable is the same, no matter
the token type. But seeing as the data type for the tokens may vary,
we must define a variable capable of holding all data types in memory.
For that, we use an _union_ (and call it YYSTYPE for some reason.):

```c
    union YYSTYPE {
        int number;
        char character;
        char* string;
        ...
    }
```

Now, because Flex cannot know beforehand what is the type of the
variable we are using for storing the token value, it doesn't
declare it when generating our .c file. We must therefore declare
it with the YYSTYPE. The given name is yylval. Don't ask me why.

```c
    union YYSTYPE yylval;
```

Now we are set. Just to sum it up:

1. A token is composed of:
   1.1. a type.
   1.2. a regex used to find it in a stream of text.
   1.3. a value (the matched text).
2. The value may be of different data types.
3. We define our token types with an enum yytokentype
4. We define the data types out token can have with an union YYSTYPE
5. We declare a global variable to hold this value yylval

### Making the scanner return tokens

Now having defined what our tokens are, we can make our scanner
return the token types when a token is matched and store the token
value in our yylval variable. The following example illustrates that:

```lex
%{
    #include <stdio.h> // included here for printf
    enum yyokentype {  // we assign numbers so that no token type is 0 (EOF)
        OP  = 99,      // this token will identify +, -, / or *
        NUM = 98,      // this token identify integers
        SPC = 97,      // special chars such as (, ), [, ], {, }
    }
    union YYSTYPE {
        int number;        // we'll store NUMBER as int
        char character;    // we'll store OP and SPC as char
    };
    union YYSTYPE yylval;  // the variable for holding the token value
%}
%option yynowrap

%%
[\n\t ]        { /* ignore tab, newline and space */          }
[0-9]+         { yylval.number = atoi(yytext); return NUM;    }
[+-/\*]        { yylval.character = *yytext;   return OP;     }
[\(\{\[\]\}\)] { yylval.character = *yytext;   return SPC;    }
.              { printf("Error: %c.", *yytext); yyterminate();}
%%

// we'll add a main function for printing tokens from a file
// the program finishes if something that is not a token is found
void main(int argc, char argv[]) {
    yyin = fopen("testfile.txt", "r"); // read stream from file instead of stdin
    int token_type;
    do {
        token_type = yylex();
        switch (token_type) {
            case OP:
                printf("Found OP token: %c", yylval.character);
                break;
            case OP:
                printf("Found NUM token: %d", yylval.number);
                break;
            case OP:
                printf("Found SPC token: %c", yylval.character);
                break;
        }
    } while(token_type != 0);  // if yylex returned 0 we reached EOF
}
```

### How Bison comes into play

As Flex is the program we use to write lexers and scan a stream
for tokens, Bison is the program we use to write parsers and build
our ASTs. The latter is by far the most complicated of the two.

When using those in conjunction, we actually write our token
definitions in a bison .y as such:

```yacc
    %union {
        int  number;
        char character;
    }
    %token <number>    NUM
    %token <character> OP SPC
```

Calling bison w/ the -d flag will generate us a my_parser.tab.h.
This file will declare and define our YYSTYPE, yytokentype and
yylval based on the .y file token definitions.

```sh
    bison -d my_parser.y
    # generates my_parser.tab.h
    # -> contains yytokentype, YYSTYPE, yylval
```

We then include this HEADER file in our .l and generate our Rules
based on the imported token definitions.

```lex
%{
    #include "my_parser.tab.h" // token definitions
%}
%%
... rules with access to yylval and yytokentype
%%
...
```

## The example in this repository

The example here defines our YYSTYPE, yylval yytokentype
manually in the lex.h file. It also makes this header
provide the yylex() function to the scanner.c file which
will run the main function. The idea here is to show that
a Flex generated lexer can be used as a library of sorts
for your sources and not necessarily be a standalone program
or part of a Bison program.
